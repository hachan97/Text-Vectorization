{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data.\n",
    "\n",
    "Download the dataset from https://surfdrive.surf.nl/files/index.php/s/bfNFkuUVoVtiyuk. This is a subset of the data from https://doi.org/10.7910/DVN/YHWTFC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Load all the articles from the Infowars directory\n",
    "infowarsfiles = glob('articles/*/Infowars/*')\n",
    "infowarsarticles = []\n",
    "\n",
    "for filename in infowarsfiles:\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            infowarsarticles.append(f.read())\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2908\n",
      "Number of unique outlets: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlets</th>\n",
       "      <th>article_text</th>\n",
       "      <th>date</th>\n",
       "      <th>article_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>A high school in Vermonts capital raised a Bla...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Black Lives Matter Flag Flies Over Vermont Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>A Monmouth Poll released on Wednesday that sho...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Democratic Generic Congressional Ballot Advant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>Thursday on Fox News Channels The Story, Rep. ...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>GOP Rep Gaetz Responds to Comey Tweet I Dont K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>Hillary Clinton has passed over a million doll...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Hillary Has Sent Over 1 Million From Onward To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>The Department of Immigration and Customs Enfo...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>ICE Raids 77 Northern California Businesses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>President Donald Trumps approval rating is now...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Trump Approval Rating Almost Tied With Obama A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>President Trump took to Twitter on Thursday ni...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Trump Democrats just arent calling on DACA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>Donald Trump has lashed out at the leadership ...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Trump Leadership of FBI Justice Dept politiciz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>The classified Republican memo that has become...</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Washington braces for polarizing Nunes memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Infowars</td>\n",
       "      <td>President Donald Trump celebrated the Friday r...</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>Collusion Is Dead Trump Celebrates Release of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlets                                       article_text        date  \\\n",
       "0  Infowars  A high school in Vermonts capital raised a Bla...  2018-02-02   \n",
       "1  Infowars  A Monmouth Poll released on Wednesday that sho...  2018-02-02   \n",
       "2  Infowars  Thursday on Fox News Channels The Story, Rep. ...  2018-02-02   \n",
       "3  Infowars  Hillary Clinton has passed over a million doll...  2018-02-02   \n",
       "4  Infowars  The Department of Immigration and Customs Enfo...  2018-02-02   \n",
       "5  Infowars  President Donald Trumps approval rating is now...  2018-02-02   \n",
       "6  Infowars  President Trump took to Twitter on Thursday ni...  2018-02-02   \n",
       "7  Infowars  Donald Trump has lashed out at the leadership ...  2018-02-02   \n",
       "8  Infowars  The classified Republican memo that has become...  2018-02-02   \n",
       "9  Infowars  President Donald Trump celebrated the Friday r...  2018-02-03   \n",
       "\n",
       "                                       article_title  \n",
       "0  Black Lives Matter Flag Flies Over Vermont Sch...  \n",
       "1  Democratic Generic Congressional Ballot Advant...  \n",
       "2  GOP Rep Gaetz Responds to Comey Tweet I Dont K...  \n",
       "3  Hillary Has Sent Over 1 Million From Onward To...  \n",
       "4        ICE Raids 77 Northern California Businesses  \n",
       "5  Trump Approval Rating Almost Tied With Obama A...  \n",
       "6         Trump Democrats just arent calling on DACA  \n",
       "7  Trump Leadership of FBI Justice Dept politiciz...  \n",
       "8        Washington braces for polarizing Nunes memo  \n",
       "9  Collusion Is Dead Trump Celebrates Release of ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define a generator function to read the data\n",
    "def read_data(listofoutlets):\n",
    "    for label in listofoutlets:\n",
    "        for file in glob(os.path.join( f'articles/*/{label}/*')):\n",
    "            with open(file) as f:\n",
    "                yield label, file, f.read()\n",
    "\n",
    "# Use the generator to read the articles and set the corresponding labels\n",
    "data = list(read_data(['Infowars', 'BBC', 'The Guardian']))\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data, columns=['outlets', 'data', 'article_text'])\n",
    "\n",
    "# Extract 'date' and 'article_title' from the 'data' column\n",
    "df['date'] = df['data'].apply(lambda x: re.search(r'--(\\d{4}-\\d{2}-\\d{2})--', x).group(1) if re.search(r'--(\\d{4}-\\d{2}-\\d{2})--', x) else None)\n",
    "df['article_title'] = df['data'].apply(lambda x: os.path.basename(x).split('--')[-1])\n",
    "\n",
    "# Drop the original 'data' column as it's no longer needed\n",
    "df.drop(columns=['data'], inplace=True)\n",
    "\n",
    "# Report outcome\n",
    "print(f\"Number of documents: {len(df)}\")\n",
    "print(f\"Number of unique outlets: {df['outlets'].nunique()}\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob  # allow to search for files and directories that match a specified pattern.\n",
    "\n",
    "# Define Function to retrieve news articles from specified news outlets\n",
    "def read_data(listofoutlets):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in listofoutlets:\n",
    "        for file in glob(f'articles/*/{label}/*'):\n",
    "            try:\n",
    "                with open(file) as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(label)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "    return texts, labels\n",
    "\n",
    "X, y = read_data(['Infowars', 'BBC', 'The Guardian']) #choose your own newsoutlets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the dataset in a train and test sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data\n",
    "\n",
    "To train a classifier that will predict whether articles come from a fake news source (e.g., `Infowars`) or a quality news outlet (e.g., `bbc`). In other words, I want to predict `source` based on linguistic variations in the articles.\n",
    "\n",
    "To arrive at a model that will do just that, I transform 'text' to 'features'. By defining different vectorizers, with the following options:\n",
    "- `count` vs. `tfidf` vectorizers\n",
    "- with/ without pruning\n",
    "- with/ without stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer\n",
    "vectorizer_TFIDF = TfidfVectorizer(tokenizer=TreebankWordTokenizer().tokenize, \n",
    "                                  stop_words='english', \n",
    "                                  max_df=.75, \n",
    "                                  min_df=2)  #remove words that occur in more than 75% or less than n = 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "vectorizer_count = CountVectorizer(tokenizer=TreebankWordTokenizer().tokenize, \n",
    "                                  stop_words='english', \n",
    "                                  max_df=.75, \n",
    "                                  min_df=2)  #remove words that occur in more than 75% or less than n = 2 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we only '.transform' the Test-set, but only '.fit_transform' the Train-set?**\n",
    "\n",
    "- fit_transform on Training Data: Learns the vocabulary and IDF values from the training data and transforms the training data into a TF-IDF matrix.\n",
    "- transform on Test Data: Uses the learned vocabulary and IDF values from the training data to transform the test data into a TF-IDF matrix, ensuring consistency and preventing data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Fit the vectorizer, and transform.\n",
    "X_TFIDFfeatures_train = vectorizer_TFIDF.fit_transform(X_train) #fit and transform the training data\n",
    "X_TFIDFfeatures_test = vectorizer_TFIDF.transform(X_test)\n",
    "\n",
    "X_countfeatures_train = vectorizer_count.fit_transform(X_train) #fit and transform the training data\n",
    "X_countfeatures_test = vectorizer_count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'backside</th>\n",
       "      <th>'bad</th>\n",
       "      <th>'blink</th>\n",
       "      <th>'blue</th>\n",
       "      <th>'but</th>\n",
       "      <th>'curious</th>\n",
       "      <th>'d</th>\n",
       "      <th>'do</th>\n",
       "      <th>'excuse</th>\n",
       "      <th>'free</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2326 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      'backside  'bad  'blink  'blue  'but  'curious   'd  'do  'excuse  'free\n",
       "0           0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "1           0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "2           0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "3           0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "4           0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "...         ...   ...     ...    ...   ...       ...  ...  ...      ...    ...\n",
       "2321        0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "2322        0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "2323        0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "2324        0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "2325        0.0   0.0     0.0    0.0   0.0       0.0  0.0  0.0      0.0    0.0\n",
       "\n",
       "[2326 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Sparse matrix\n",
    "import pandas as pd\n",
    "X_features_train_df = pd.DataFrame(X_features_train.toarray(), \n",
    "                                   columns=vectorizer_TFIDF.get_feature_names_out())\n",
    "\n",
    "X_features_train_df.iloc[:, 10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Classifier - Compare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #import the Multinomial Naive Bayes model classifier model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.865979381443299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BBC       0.93      0.96      0.95       202\n",
      "    Infowars       0.93      0.73      0.82       221\n",
      "The Guardian       0.74      0.93      0.83       159\n",
      "\n",
      "    accuracy                           0.87       582\n",
      "   macro avg       0.87      0.87      0.86       582\n",
      "weighted avg       0.88      0.87      0.87       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics of TfidVectorizer\n",
    "model = MultinomialNB()\n",
    "model.fit(X_TFIDFfeatures_train, y_train) #Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(X_TFIDFfeatures_test) \n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8848797250859106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BBC       0.97      0.96      0.96       202\n",
      "    Infowars       0.93      0.78      0.85       221\n",
      "The Guardian       0.76      0.94      0.84       159\n",
      "\n",
      "    accuracy                           0.88       582\n",
      "   macro avg       0.89      0.89      0.88       582\n",
      "weighted avg       0.90      0.88      0.89       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics of CountVectorizer\n",
    "model = MultinomialNB()\n",
    "model.fit(X_countfeatures_train, y_train) #Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(X_countfeatures_test)\n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.929553264604811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BBC       1.00      0.91      0.95       202\n",
      "    Infowars       0.88      0.95      0.91       221\n",
      "The Guardian       0.92      0.93      0.93       159\n",
      "\n",
      "    accuracy                           0.93       582\n",
      "   macro avg       0.93      0.93      0.93       582\n",
      "weighted avg       0.93      0.93      0.93       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics of TfidVectorizer\n",
    "model = LogisticRegression()\n",
    "model.fit(X_TFIDFfeatures_train, y_train) #Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(X_TFIDFfeatures_test)\n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9432989690721649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BBC       0.96      0.98      0.97       202\n",
      "    Infowars       0.93      0.93      0.93       221\n",
      "The Guardian       0.94      0.92      0.93       159\n",
      "\n",
      "    accuracy                           0.94       582\n",
      "   macro avg       0.94      0.94      0.94       582\n",
      "weighted avg       0.94      0.94      0.94       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics of CountVectorizer\n",
    "model = LogisticRegression()\n",
    "model.fit(X_countfeatures_train, y_train) #Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(X_countfeatures_test)\n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
